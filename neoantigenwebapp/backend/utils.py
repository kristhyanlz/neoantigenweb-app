from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_auc_score

# queda pendiente verificar el calculo correcto del AUC, para esto se necesita la probalidades

def compute_metrics(pred):
    labels = pred.label_ids
    prediction=pred.predictions

    print(labels, prediction)

    preds = prediction.argmax(-1)
    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()
    precision = tp / (tp + fp) 
    recall = tp / (tp + fn)
    sn = tp / (tp + fp)       
    sp = tn / (tn + fp)  # true negative rate
    mcc = matthews_corrcoef(labels, preds)
    acc = accuracy_score(labels, preds)
    auc = roc_auc_score(labels, preds)
    f1 = 2 * (precision * recall) / (precision + recall)
    return {
        'auc': auc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'sn': sn,
        'sp': sp,
        'accuracy': acc,
        'mcc': mcc
    }
